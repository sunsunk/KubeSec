// Module included in the following assemblies:
//
// assembly-scaling-clusters.adoc

[id='con-scaling-kafka-clusters-{context}']
= Scaling clusters by adding or removing brokers

[role="_abstract"]
Scaling Kafka clusters by adding brokers can increase the performance and reliability of the cluster. 
Adding more brokers increases available resources, allowing the cluster to handle larger workloads and process more messages. 
It can also improve fault tolerance by providing more replicas and backups. 
Conversely, removing underutilized brokers can reduce resource consumption and improve efficiency. 
Scaling must be done carefully to avoid disruption or data loss.
By redistributing partitions across all brokers in the cluster, the resource utilization of each broker is reduced, which can increase the overall throughput of the cluster.

NOTE: To increase the throughput of a Kafka topic, you can increase the number of partitions for that topic. 
This allows the load of the topic to be shared between different brokers in the cluster. 
However, if every broker is constrained by a specific resource (such as I/O), adding more partitions will not increase the throughput. 
In this case, you need to add more brokers to the cluster.

Adjusting the `Kafka.spec.kafka.replicas` configuration affects the number of brokers in the cluster that act as replicas. 
The actual replication factor for topics is determined by settings for the `default.replication.factor` and `min.insync.replicas`, and the number of available brokers. 
For example, a replication factor of 3 means that each partition of a topic is replicated across three brokers, ensuring fault tolerance in the event of a broker failure.

.Example replica configuration
[source,yaml,subs="+attributes"]
----
apiVersion: {KafkaApiVersion}
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    replicas: 3
    # ...
  config:
      # ...
      default.replication.factor: 3
      min.insync.replicas: 2  
 # ...
----

When adding brokers through the `Kafka` resource configuration, node IDs start at 0 (zero) and the Cluster Operator assigns the next lowest ID to a new node. 
The broker removal process starts from the broker pod with the highest ID in the cluster.

If you are managing nodes in the cluster using node pools, adjust the `KafkaNodePool.spec.replicas` configuration to change the number of nodes in the node pool.
Additionally, when scaling existing clusters with node pools, you can xref:proc-managing-node-pools-ids-{context}[assign node IDs for the scaling operations].  

When you add add or remove brokers, Kafka does not automatically reassign partitions. 
The best way to do this is using Cruise Control.
You can use Cruise Control's `add-brokers` and `remove-brokers` modes when scaling a cluster up or down.

* Use the `add-brokers` mode after scaling up a Kafka cluster to move partition replicas from existing brokers to the newly added brokers.
* Use the `remove-brokers` mode before scaling down a Kafka cluster to move partition replicas off the brokers that are going to be removed.

include::../../modules/configuring/con-skipping-scale-down-checks.adoc[leveloffset=+1]