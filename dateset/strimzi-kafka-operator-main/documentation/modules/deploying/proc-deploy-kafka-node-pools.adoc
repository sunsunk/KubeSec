// Module included in the following assemblies:
//
// deploying/assembly_deploy-kafka-cluster.adoc

[id='deploying-kafka-node-pools-{context}']
= Deploying a Kafka cluster with node pools

[role="_abstract"]
This procedure shows how to deploy Kafka with node pools to your Kubernetes cluster using the Cluster Operator.
Node pools represent a distinct group of Kafka nodes within a Kafka cluster that share the same configuration.
For each Kafka node in the node pool, any configuration not defined in node pool is inherited from the cluster configuration in the `kafka` resource.

The deployment uses a YAML file to provide the specification to create a `KafkaNodePool` resource.
You can use node pools with Kafka clusters that use KRaft (Kafka Raft metadata) mode or ZooKeeper for cluster management.
To deploy a Kafka cluster in KRaft mode, you must use the `KafkaNodePool` resources.

Strimzi provides the following xref:config-examples-{context}[example files] that you can use to create a Kafka cluster that uses node pools:

`kafka-with-dual-role-kraft-nodes.yaml`:: Deploys a Kafka cluster with one pool of KRaft nodes that share the broker and controller roles.
`kafka-with-kraft.yaml`:: Deploys a persistent Kafka cluster with one pool of controller nodes and one pool of broker nodes.
`kafka-with-kraft-ephemeral.yaml`:: Deploys an ephemeral Kafka cluster with one pool of controller nodes and one pool of broker nodes.
`kafka.yaml`:: Deploys ZooKeeper with 3 nodes, and 2 different pools of Kafka brokers. Each of the pools has 3 brokers. The pools in the example use different storage configuration.

NOTE: You can perform the steps outlined here to deploy a new Kafka cluster with `KafkaNodePool` resources or xref:proc-migrating-clusters-node-pools-{context}[migrate your existing Kafka cluster].  

.Prerequisites

* xref:deploying-cluster-operator-str[The Cluster Operator must be deployed.]  

.Procedure

. Deploy a KRaft-based Kafka cluster.
+
* To deploy a Kafka cluster in KRaft mode with a single node pool that uses dual-role nodes:
+
[source,shell,subs="attributes+"]
kubectl apply -f examples/kafka/kraft/kafka-with-dual-role-nodes.yaml

* To deploy a persistent Kafka cluster in KRaft mode with separate node pools for broker and controller nodes: 
+
[source,shell,subs="attributes+"]
kubectl apply -f examples/kafka/kraft/kafka.yaml

* To deploy an ephemeral Kafka cluster in KRaft mode with separate node pools for broker and controller nodes: 
+
[source,shell,subs="attributes+"]
kubectl apply -f examples/kafka/kraft/kafka-ephemeral.yaml

* To deploy a Kafka cluster and ZooKeeper cluster with two node pools of three brokers:
+
[source,shell,subs="attributes+"]
kubectl apply -f examples/kafka/kafka-with-node-pools.yaml

. Check the status of the deployment:
+
[source,shell,subs="+quotes"]
----
kubectl get pods -n _<my_cluster_operator_namespace>_
----
+
.Output shows the node pool names and readiness
[source,shell,subs="+quotes"]
----
NAME                        READY  STATUS   RESTARTS
my-cluster-entity-operator  3/3    Running  0
my-cluster-pool-a-0         1/1    Running  0
my-cluster-pool-a-1         1/1    Running  0
my-cluster-pool-a-4         1/1    Running  0
----
+
* `my-cluster` is the name of the Kafka cluster.
* `pool-a` is the name of the node pool.
+
A sequential index number starting with `0` identifies each Kafka pod created.
If you are using ZooKeeper, you'll also see the ZooKeeper pods.
+
`READY` shows the number of replicas that are ready/expected.
The deployment is successful when the `STATUS` displays as `Running`.
+
Information on the deployment is also shown in the status of the `KafkaNodePool` resource, including a list of IDs for nodes in the pool.
+
NOTE: Node IDs are assigned sequentially starting at 0 (zero) across all node pools within a cluster. This means that node IDs might not run sequentially within a specific node pool. If there are gaps in the sequence of node IDs across the cluster, the next node to be added is assigned an ID that fills the gap. When scaling down, the node with the highest node ID within a pool is removed.

[role="_additional-resources"]
.Additional resources

xref:config-node-pools-{context}[Node pool configuration]