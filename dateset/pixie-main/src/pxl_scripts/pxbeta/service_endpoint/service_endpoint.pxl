# Copyright 2018- The Pixie Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

''' Endpoint Overview

 This script gets an overview of an individual endpoint on an
 individual service, summarizing its request statistics.
'''
import px

ns_per_ms = 1000 * 1000
ns_per_s = 1000 * ns_per_ms
# Window size to use on time_ column for bucketing.
window_ns = px.DurationNanos(10 * ns_per_s)
# Flag to filter out requests that come from an unresolvable IP.
filter_unresolved_inbound = False
# Flag to filter out health checks from the data.
filter_health_checks = False
# Flag to filter out ready checks from the data.
filter_ready_checks = False


def endpoint_let_timeseries(start_time: str, service: px.Service, endpoint: str):
    ''' Compute the let as a timeseries for requests received by `service` at `endpoint`.

    Args:
    @start_time: The timestamp of data to start at.
    @service: The name of the service to filter on.
    @endpoint: The endpoint to filter on.

    '''
    df = let_helper(start_time, service)
    df = df[px._match_endpoint(df.req_path, endpoint)]

    df = df.groupby(['timestamp']).agg(
        latency_quantiles=('latency', px.quantiles),
        error_rate_per_window=('failure', px.mean),
        throughput_total=('latency', px.count),
        bytes_total=('resp_body_size', px.sum)
    )

    # Format the result of LET aggregates into proper scalar formats and
    # time series.
    df.latency_p50 = px.DurationNanos(
        px.floor(px.pluck_float64(df.latency_quantiles, 'p50')))
    df.latency_p90 = px.DurationNanos(
        px.floor(px.pluck_float64(df.latency_quantiles, 'p90')))
    df.latency_p99 = px.DurationNanos(
        px.floor(px.pluck_float64(df.latency_quantiles, 'p99')))
    df.request_throughput = df.throughput_total / window_ns
    df.errors_per_ns = df.error_rate_per_window * \
        df.request_throughput / px.DurationNanos(1)
    df.error_rate = px.Percent(df.error_rate_per_window)
    df.bytes_per_ns = df.bytes_total / window_ns
    df.time_ = df.timestamp

    return df[['time_', 'latency_p50', 'latency_p90', 'latency_p99',
               'request_throughput', 'errors_per_ns', 'error_rate', 'bytes_per_ns']]


def endpoint_slow_requests(start_time: str, service: px.Service, endpoint: str):
    df = let_helper(start_time, service)
    df = df[px._match_endpoint(df.req_path, endpoint)]
    df = add_source_dest_columns(df)
    df = add_source_dest_links(df, start_time)

    quantiles = df.groupby('service').agg(
        latency_quantiles=('latency', px.quantiles)
    )
    quantiles.service_p99 = px.pluck_float64(
        quantiles.latency_quantiles, 'p99')
    quantiles = quantiles.drop('latency_quantiles')
    requests = df.merge(quantiles, left_on='service', right_on='service', how='inner',
                        suffixes=['', '_x'])
    requests = requests[requests.latency >= px.floor(requests.service_p99)]
    return requests[['time_', 'source', 'latency', 'req_method',
                     'req_path', 'req_body', 'resp_status', 'resp_body']].head(100)


def let_helper(start_time: str, service: px.Service):
    ''' Compute the initial part of the let for requests.

    Args:
    @start_time: The timestamp of data to start at.
    @service: The service to filter on.

    '''
    df = px.DataFrame(table='http_events', start_time=start_time)

    # Filter only to inbound service traffic (server-side).
    # Don't include traffic initiated by this service to an external location.
    df = df[df.trace_role == 2]

    df.service = df.ctx['service']
    df = df[px.contains(df.service, service)]
    df.pod = df.ctx['pod']
    df.timestamp = px.bin(df.time_, window_ns)
    df.failure = df.resp_status >= 400

    filter_out_conds = ((df.req_path != '/healthz' or not filter_health_checks) and (
        df.req_path != '/readyz' or not filter_ready_checks)) and (
        df['remote_addr'] != '-' or not filter_unresolved_inbound)
    df = df[filter_out_conds]

    return df


def add_source_dest_columns(df):
    ''' Add source and destination columns.

    Connections are traced server-side (trace_role==2), unless the server is outside
    of the cluster in which case the connection is traced client-side (trace_role==1).

    With trace_role==2: source is the `remote_addr` col, destination is the `pod` col.
    With trace_role==1: source is the `pod` col, destination is the `remote_addr` col.

    Input DataFrame must contain trace_role, upid, remote_addr columns.
    '''
    df.pod = df.ctx['pod']
    df.namespace = df.ctx['namespace']

    # If remote_addr is a pod, get its name. If not, use IP address.
    df.ra_pod = px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_addr))
    df.is_ra_pod = df.ra_pod != ''
    df.ra_name = px.select(df.is_ra_pod, df.ra_pod, df.remote_addr)

    df.is_server_tracing = df.trace_role == 2
    df.is_source_pod_type = px.select(df.is_server_tracing, df.is_ra_pod, True)
    df.is_dest_pod_type = px.select(df.is_server_tracing, True, df.is_ra_pod)

    # Set source and destination based on trace_role.
    df.source = px.select(df.is_server_tracing, df.ra_name, df.pod)
    df.destination = px.select(df.is_server_tracing, df.pod, df.ra_name)

    # Filter out messages with empty source / destination.
    df = df[df.source != '']
    df = df[df.destination != '']

    df = df.drop(['ra_pod', 'is_ra_pod', 'ra_name', 'is_server_tracing'])

    return df


def add_source_dest_links(df, start_time: str):
    ''' Modifies the source and destination columns to display deeplinks in the UI.
    Clicking on a pod name in either column will run the px/pod script for that pod.
    Clicking on an IP address, will run the px/net_flow_graph script showing all
    network connections to/from that address.

    Input DataFrame must contain source, destination, is_source_pod_type,
    is_dest_pod_type, and namespace columns.
    '''

    # Source linking. If source is a pod, link to px/pod. If an IP addr, link to px/net_flow_graph.
    df.src_pod_link = px.script_reference(df.source, 'px/pod', {
        'start_time': start_time,
        'pod': df.source
    })
    df.src_link = px.script_reference(df.source, 'px/net_flow_graph', {
        'start_time': start_time,
        'namespace': df.namespace,
        'from_entity_filter': df.source,
        'to_entity_filter': '',
        'throughput_filter': '0.0'
    })
    df.source = px.select(df.is_source_pod_type, df.src_pod_link, df.src_link)

    # If destination is a pod, link to px/pod. If an IP addr, link to px/net_flow_graph.
    df.dest_pod_link = px.script_reference(df.destination, 'px/pod', {
        'start_time': start_time,
        'pod': df.destination
    })
    df.dest_link = px.script_reference(df.destination, 'px/net_flow_graph', {
        'start_time': start_time,
        'namespace': df.namespace,
        'from_entity_filter': '',
        'to_entity_filter': df.destination,
        'throughput_filter': '0.0'
    })
    df.destination = px.select(df.is_dest_pod_type, df.dest_pod_link, df.dest_link)

    df = df.drop(['src_pod_link', 'src_link', 'is_source_pod_type', 'dest_pod_link',
                  'dest_link', 'is_dest_pod_type'])

    return df
