# Copyright 2018- The Pixie Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

# Copyright (c) Pixie Labs, Inc.
# Licensed under the Apache License, Version 2.0 (the "License")

'''
Normalized SQL Queries

This live view calculates the latency, error rate, and throughput
of each distinct normalized SQL Query.
'''

import px

# ----------------------------------------------------------------
# Visualization Variables - No need to edit for basic configuration.
# ----------------------------------------------------------------
ns_per_ms = 1000 * 1000
ns_per_s = 1000 * ns_per_ms
# Window size to use on time_ column for bucketing.
window_ns = px.DurationNanos(10 * ns_per_s)
# The bin size to use for the latency histogram.
latency_bin_size_ns = px.DurationNanos(50 * ns_per_ms)
# ----------------------------------------------------------------


# ----------------------------------------------------------------
# Visualization functions:
#
# These functions are formatted and ready for use in
# the visualization specification, vis.json.
# ----------------------------------------------------------------
def pod_sql_let(start_time: str, pod: px.Pod, normed_query: str):
    ''' Calculate LET time-series for PostgreSQL or MySQL traffic
    for a given normalized SQL query.

    Calculates latency and throughput for each parameter set to the query.

    @start_time The timestamp of data to start at.
    @pod: the partial/full-name of the pod to monitor LET.
    @normed_query: the normalized SQL query to monitor.

    Returns: Returns the DataFrame containing LET time-series for
        traffic to a PostgreSQL or MySQL database pod grouped by query params.
    '''
    df = merged_let_per_pod(start_time, pod)
    df = df[df.normed_query == normed_query]

    return df[['time_', 'normed_query', 'params', 'latency_p50',
              'latency_p90', 'latency_p99', 'request_throughput']]


def summary_sql_let(start_time: str, pod: px.Pod, normed_query: str):
    ''' Calculate LET summary for PostgreSQL or MySQL traffic
    for a given normalized SQL query.

    Calculates latency and throughput for each parameter set to the query.

    @start_time The timestamp of data to start at.
    @pod: the partial/full-name of the pod to monitor LET.
    @normed_query: the normalized SQL query to monitor.

    Returns: Returns the DataFrame containing LET summary statistics
    for the query grouped by parameter set.
    '''

    df = merged_let_per_pod(start_time, pod)
    df = df[df.normed_query == normed_query]
    df = summarize_LET(df, ['normed_query', 'params'])

    return df[['params', 'normed_query',
               'request_throughput', 'latency', 'total_requests']]


# ----------------------------------------------------------------
# Utility functions:
#
# These are shared functions. We plan to support imports in v0.3,
# which will allow these functions to be shared across multiple
# scripts.
# ----------------------------------------------------------------
def merged_let_per_pod(start_time: str, pod: px.Pod):
    pgsql_df = normalized_pgsql_per_pod(start_time, pod)
    mysql_df = normalized_mysql_per_pod(start_time, pod)

    common_cols = ['timestamp', 'normed_query', 'params', 'latency']
    pgsql_df = pgsql_df[common_cols]
    mysql_df = mysql_df[common_cols]

    # we don't apriori know whether a pod is a postgres or mysql pod,
    # so we merge the two event tables together
    # and then calculate our LET timeseries.
    df = pgsql_df.append(mysql_df)

    # TODO(james, PP-2582): once we have list addition we can make
    # groups an argument to this function, similar to how it works
    # in mysql_stats/pgsql_stats.
    # Without list addition we can't do this cause we need
    # to determine column_cols as groups + ['latency']
    groups = ['timestamp', 'normed_query', 'params']
    df = calc_LET(df, groups)
    return df


def normalized_pgsql_per_pod(start_time: str, pod: px.Pod):
    ''' Normalized traffic per connection to a PostgreSQL database pod.

    Formats the table and normalizes queries.

    @start_time The timestamp of data to start at.
    @pod: the partial/full-name of the pod to monitor PostgreSQL LET.

    Returns: Returns the DataFrame containing normalized queries
    and unaggregated LET statistics.
    '''
    # The data necessary to compute PostgreSQL LET information is located in the
    # pgsql_events table. We filter and aggregate data
    # from this table to compute the required metrics.
    df = px.DataFrame(table='pgsql_events', start_time=start_time)
    df = format_pgsql_table(df)

    # Calculate LET of pod(s) (k8s_object) connection to PostgreSQL connections
    # over the time window ('timestamp') after filtering for matching svcs.
    df = df[px.contains(df.source, pod) or px.contains(df.destination, pod)]

    df = filter_pgsql_events(df)
    df = normalize_pgsql(df)
    return df


def normalized_mysql_per_pod(start_time: str, pod: px.Pod):
    ''' Normalized MySQL traffic per connection to a MySQL database pod.

    Formats the table and normalizes queries.

    @start_time The timestamp of data to start at.
    @pod: the partial/full-name of the pod to monitor MySQL LET.

    Returns: Returns the DataFrame containing normalized MySQL queries
    and unaggregated LET statistics.
    '''
    # The data necessary to compute MySQL LET information is located in the
    # mysql_events table. We filter and aggregate data
    # from this table to compute the required metrics.
    df = px.DataFrame(table='mysql_events', start_time=start_time)
    df = format_mysql_table(df, False)

    # Calculate LET of pod(s) (k8s_object) connection to PostgreSQL connections
    # over the time window ('timestamp') after filtering for matching svcs.
    df = df[px.contains(df.source, pod) or px.contains(df.destination, pod)]

    df = filter_mysql_events(df)
    df = normalize_mysql(df)
    return df


def format_events_table(df, latency_col):
    ''' Format data and add semantic columns in event tables

    Unifies latency column to 'latency_ms', adds a binned
    timestamp field to aggregate on, and adds the svc
    (k8s_object) as a semantic column.

    Works on 'pgsql_events' and 'http_events'

    Args:
    @df: the input events table
    @latency_col: the name of the latency column in @df.

    Returns: formatted events DataFrame
    '''
    df.latency = df[latency_col]
    df.timestamp = px.bin(df.time_, window_ns)
    df = df[df['pod'] != '']

    return df


def format_pgsql_table(df):
    ''' Formats pgsql_events tables

    Args:
    @df: the input pgsql_events table.

    Returns: formatted pgsql_events DataFrame.
    '''
    # Add source, destination columns.
    df = add_source_dest_columns(df)

    df = format_events_table(df, 'latency')
    return df


def format_mysql_table(df, filter_responses_with_none_code):
    ''' Formats mysql_events tables

    Runs events table universal formatting, creates a failure field
    marking which requests receive an error status code, and
    optionally filters out responses with "None" response
    error code if @filter_responses_with_none_code is true.

    Args:
    @df: the input mysql_events table.
    @filter_responses_with_none_code: flag to filter out MySQL
        responses where no data is returned.
    Returns: formatted mysql_events DataFrame.
    '''
    # Add source, destination columns.
    df = add_source_dest_columns(df)

    df = format_events_table(df, 'latency')
    df.failure = df['resp_status'] == 3

    # None response code returns on close() of MySQL connection.
    none_response_code = 1
    df = df[df.resp_status
            != none_response_code or not filter_responses_with_none_code]
    return df


def filter_pgsql_events(df):
    ''' Filters out irrelevant PostgresSQL events.

    For now we only consider QUERY and EXECUTE events.
    Args:
    @df: the input pgsql_events table.
    Returns: filtered table.
    '''
    return df[df.req_cmd == 'Execute' or df.req_cmd == 'Query']


def filter_mysql_events(df):
    ''' Filters out irrelevant MySQL events.

    For now we only consider COM_QUERY and COM_STMT_EXECUTE events.
    Args:
    @df: the input mysql_events table.
    Returns: filtered table.
    '''
    # COM_QUERY has command code 3 and COM_STMT_EXECUTE has command code 23.
    return df[df.req_cmd == 3 or df.req_cmd == 23]


def format_LET_aggs(df):
    ''' Converts the result of LET windowed aggregates into expected metrics.

    Converts the result of aggregates on windows into well-formatted metrics
    that can be visualized. Latency quantile values need to be extracted
    from the quantiles struct, and then request_throughput is calculated
    as a function of window size.


    This function represents logic shared by LET calculators for PostgreSQL and
    HTTP events.

    Args:
    @df: the input events table grouped into windows with aggregated
        columns 'throughput_total' and 'request_throughput'

    Returns: DataFrame with formatted LET metrics.
    '''
    df.latency_p50 = px.DurationNanos(px.floor(
        px.pluck_float64(df.latency_quantiles, 'p50')))
    df.latency_p90 = px.DurationNanos(px.floor(
        px.pluck_float64(df.latency_quantiles, 'p90')))
    df.latency_p99 = px.DurationNanos(px.floor(
        px.pluck_float64(df.latency_quantiles, 'p99')))
    df['time_'] = df['timestamp']
    df.request_throughput = df.throughput_total / window_ns

    return df


def calc_LET(df, groups):
    ''' Calculates Latency, Error Rate, and Throughput on PostgreSQL events.

    Calculates latency, error rate, and throughput aggregated over
    @groups.

    Args:
    @df: the input pgsql_events table.
    @groups: the list of columns to group on. 'timestamp' must be a a group
        or this will fail.

    Returns: The LET DataFrame.
    '''
    # All requests for errors and throughput
    df = df.groupby(groups).agg(
        latency_quantiles=('latency', px.quantiles),
        throughput_total=('latency', px.count)
    )

    # Format the result of LET aggregates into proper scalar formats and
    # time series.
    df = format_LET_aggs(df)
    return df


def normalize_pgsql(df):
    # Normalize queries
    df.normed_query_struct = px.normalize_pgsql(df.req, df.req_cmd)
    df.normed_query = px.pluck(df.normed_query_struct, 'query')
    df.params = px.pluck(df.normed_query_struct, 'params')

    df = df[df.normed_query != ""]
    df = df.drop(['normed_query_struct'])
    return df


def normalize_mysql(df):
    df.normed_query_struct = px.normalize_mysql(df.req_body, df.req_cmd)
    df.normed_query = px.pluck(df.normed_query_struct, 'query')
    df.params = px.pluck(df.normed_query_struct, 'params')

    df = df[df.normed_query != ""]
    df = df.drop(['normed_query_struct'])
    return df


def summarize_LET(let_df, groups):
    ''' Aggregate LET values across all windows.

    Args:
    @let_df: the DataFrame with LET values.
    @groups: the columns to group over.

    Returns: The summary DF.
    '''
    df = let_df.groupby(groups).agg(
        request_throughput=('request_throughput', px.mean),
        total_requests=('throughput_total', px.sum),
        latency=('latency_p50', px.mean)
    )
    return df


def add_source_dest_columns(df):
    ''' Add source and destination columns for the PostgreSQL request.

    PostgreSQL requests are traced server-side (trace_role==2),
    unless the server is outside of the cluster in which case the request
    is traced client-side (trace_role==1).

    When trace_role==2, the PostgreSQL request source is the remote_addr column
    and destination is the pod column. When trace_role==1,
    the PostgreSQL request source is the pod column and the destination
    is the remote_addr column.

    Input DataFrame must contain trace_role, upid, remote_addr columns.
    '''
    df.pod = df.ctx['pod']
    df.namespace = df.ctx['namespace']

    # If remote_addr is a pod, get its name. If not, use IP address.
    df.ra_pod = px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_addr))
    df.is_ra_pod = df.ra_pod != ''
    df.ra_name = px.select(df.is_ra_pod, df.ra_pod, df.remote_addr)

    df.is_server_tracing = df.trace_role == 2
    df.is_source_pod_type = px.select(df.is_server_tracing, df.is_ra_pod, True)
    df.is_dest_pod_type = px.select(df.is_server_tracing, True, df.is_ra_pod)

    # Set source and destination based on trace_role.
    df.source = px.select(df.is_server_tracing, df.ra_name, df.pod)
    df.destination = px.select(df.is_server_tracing, df.pod, df.ra_name)

    # Filter out messages with empty source / destination.
    df = df[df.source != '']
    df = df[df.destination != '']

    df = df.drop(['ra_pod', 'is_ra_pod', 'ra_name', 'is_server_tracing'])

    return df
