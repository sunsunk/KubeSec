# Copyright 2018- The Pixie Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

import px


def net_flow_graph(start_time: str, ns: px.Namespace, from_entity_filter: str,
                   to_entity_filter: str, throughput_filter: float):
    df = px.DataFrame('conn_stats', start_time=start_time)

    # Filter on namespace.
    df = df[df.ctx['namespace'] == ns]

    # Filter for client side requests.
    df = df[df.trace_role == 1]

    # Store the pod. Ideally this would be done after the aggregate,
    # but that's not working right now.
    df.pod = df.ctx['pod']

    # Filter out any non k8s sources.
    df = df[df.pod != '']

    # Find the time window
    time_window = df.agg(
        time_min=('time_', px.min),
        time_max=('time_', px.max),
    )
    time_window.time_delta = px.DurationNanos(time_window.time_max - time_window.time_min)
    time_window = time_window.drop(['time_min', 'time_max'])

    # Use aggregate to pick the first and last sample for any given client-server pair.
    # We do this by picking the min/max of the stats, since they are all counters.
    df = df.groupby(['pod', 'upid', 'remote_addr']).agg(
        bytes_sent_min=('bytes_sent', px.min),
        bytes_sent_max=('bytes_sent', px.max),
        bytes_recv_min=('bytes_recv', px.min),
        bytes_recv_max=('bytes_recv', px.max),
    )
    df.bytes_sent = df.bytes_sent_max - df.bytes_sent_min
    df.bytes_recv = df.bytes_recv_max - df.bytes_recv_min
    df.bytes_total = df.bytes_sent + df.bytes_recv
    df = df.drop(['bytes_sent_max', 'bytes_sent_min', 'bytes_recv_max', 'bytes_recv_min'])

    # To create a graph, add 'from' and 'to' entities.
    df.from_entity = df.pod

    # TODO(yzhao): Handle IPv6 ::1 as well.
    localhost_ip_regexp = r'127\.0\.0\.[0-9]+'
    df.is_remote_addr_localhost = px.regex_match(localhost_ip_regexp, df.remote_addr)
    df.to_entity = px.select(df.is_remote_addr_localhost,
                             df.pod,
                             px.nslookup(df.remote_addr))

    # Filter out entities as specified by the user.
    df = df[px.contains(df.from_entity, from_entity_filter)]
    df = df[px.contains(df.to_entity, to_entity_filter)]

    # Since there may be multiple processes per pod,
    # perform an additional aggregation to consolidate those into one entry.
    df = df.groupby(['from_entity', 'to_entity']).agg(
        bytes_sent=('bytes_sent', px.sum),
        bytes_recv=('bytes_recv', px.sum),
        bytes_total=('bytes_total', px.sum),
    )

    # Add time_delta to every row. Use a join to do this.
    # Future syntax will support: df.time_delta = time_window.at[0, 'time_delta']
    df.join_key = 1
    time_window.join_key = 1
    df = df.merge(time_window, how='inner', left_on='join_key', right_on='join_key')
    df = df.drop(['join_key_x', 'join_key_y'])

    # Compute as rates.
    df.bytes_sent = df.bytes_sent / df.time_delta
    df.bytes_recv = df.bytes_recv / df.time_delta
    df.bytes_total = df.bytes_total / df.time_delta
    df = df.drop(['time_delta'])

    # Apply rate filter.
    df = df[df.bytes_total > throughput_filter / 1000000000]

    return df
